{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Machine Learning Project using K-Nearest Neighbors (KNN)\n",
    "\n",
    "In this notebook, we demonstrate the KNN algorithm for classification tasks. We cover the complete machine learning workflow, including data loading, exploratory data analysis (EDA), data preprocessing, mathematical explanation, model training and evaluation, model analysis & visualization, discussion, conclusion, and references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a simple, non-parametric, and instance-based learning method used for both classification and regression. It works on the assumption that similar instances exist in close proximity in the feature space. KNN is significant because:\n",
    "\n",
    "- It is easy to implement and understand.\n",
    "- It makes no assumptions about the underlying data distribution.\n",
    "\n",
    "KNN finds applications in various domains such as product recommendation systems, fraud detection, and customer segmentation. In this notebook, we will use the well-known Iris dataset to illustrate the KNN workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description & Exploratory Analysis\n",
    "\n",
    "We will use the Iris dataset, which contains measurements for three species of Iris flowers. The dataset includes the features: sepal length, sepal width, petal length, and petal width, along with the target class (species).\n",
    "\n",
    "In this section, we:\n",
    "\n",
    "- Load the dataset\n",
    "- Display basic statistical summaries\n",
    "- Check for missing values\n",
    "- Visualize the data using pair plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load Iris dataset from sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "data['target'] = iris.target\n",
    "data['species'] = data['target'].map(dict(zip(range(3), iris.target_names)))\n",
    "\n",
    "# Display the first 5 rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(data['species'].value_counts())\n",
    "sns.countplot(x='species', data=data)\n",
    "plt.title('Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistical summary\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the dataset\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot to visualize relationships between features\n",
    "sns.pairplot(data, hue='species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "In this section, we prepare the data for model training. The steps include:\n",
    "\n",
    "- Handling missing values (if any)\n",
    "- Scaling features using normalization (StandardScaler)\n",
    "- (If needed) Encoding categorical variables\n",
    "- Splitting the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for splitting and scaling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data[iris.feature_names]\n",
    "y = data['target']\n",
    "\n",
    "# Split dataset: 80% training and 20% testing (using stratification)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is crucial for KNN because it is a distance-based algorithm. Features with larger scales can dominate the distance calculation, so we use StandardScaler to normalize all features to have mean 0 and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the feature values using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Training and testing sets have been prepared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Explanation\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm classifies a new instance based on the classes of its k nearest neighbors. The key mathematical concepts are as follows:\n",
    "\n",
    "1. **Distance Calculation:**\n",
    "   - **Euclidean Distance:** For two points, $ x $ and $ y $, in an n-dimensional space:\n",
    "     \n",
    "     $ d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $\n",
    "\n",
    "   - **Manhattan Distance:** The distance is calculated as:\n",
    "     \n",
    "     $ d(x, y) = \\sum_{i=1}^{n} |x_i - y_i| $\n",
    "\n",
    "2. **Choice of k (Number of Neighbors):**\n",
    "   - A small k value may make the model sensitive to noise (overfitting), while a large k may smooth out the decision boundary (underfitting).\n",
    "\n",
    "3. **Majority Voting:**\n",
    "   - The class label for the new instance is determined by the majority class among its k nearest neighbors.\n",
    "\n",
    "Both the distance metric and the choice of k play a crucial role in the modelâ€™s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training & Evaluation\n",
    "\n",
    "In this section, we train the KNN model using Scikit-learn, tune hyperparameters (such as k and the distance metric), and evaluate performance using metrics like accuracy, precision, recall, F1-score, and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the KNN classifier and evaluation metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Initialize the KNN classifier with an initial k value (e.g., 5) using Euclidean distance (Minkowski with p=2)\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "\n",
    "# Train the model on the scaled training data\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict target labels on the test set\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the model and show as DataFrame\n",
    "metrics = {\n",
    "    \"Accuracy\": [accuracy_score(y_test, y_pred)],\n",
    "    \"Precision\": [precision_score(y_test, y_pred, average='weighted')],\n",
    "    \"Recall\": [recall_score(y_test, y_pred, average='weighted')],\n",
    "    \"F1 Score\": [f1_score(y_test, y_pred, average='weighted')]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Confusion Matrix using Seaborn\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Hyperparameter tuning using GridSearchCV to find the best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'metric': ['euclidean', 'manhattan'],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best estimator on the test set\n",
    "best_knn = grid_search.best_estimator_\n",
    "y_pred_best = best_knn.predict(X_test_scaled)\n",
    "\n",
    "acc_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Test set accuracy of best model: {acc_best:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show full classification report for the best model\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification report for best model:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis & Visualization\n",
    "\n",
    "Here, we further analyze the model by visualizing decision boundaries and the effect of different k values on performance. Since decision boundary plots are easier in two dimensions, we reduce our dataset to two features (sepal length and sepal width) for these visualizations.\n",
    "\n",
    "The following visualizations will be generated:\n",
    "\n",
    "- **Decision Boundary Plot**: Shows the classifierâ€™s separation of classes.\n",
    "- **K-value Selection Visualization**: Illustrates test accuracy as a function of the number of neighbors (k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For decision boundary visualization, select two features: sepal length and sepal width\n",
    "features = ['sepal length (cm)', 'sepal width (cm)']\n",
    "\n",
    "X_vis = X[features]\n",
    "y_vis = y\n",
    "\n",
    "# Split the data (using stratification for consistency)\n",
    "X_train_vis, X_test_vis, y_train_vis, y_test_vis = train_test_split(X_vis, y_vis, test_size=0.2, random_state=42, stratify=y_vis)\n",
    "\n",
    "# Scale the selected features\n",
    "scaler_vis = StandardScaler()\n",
    "X_train_vis_scaled = scaler_vis.fit_transform(X_train_vis)\n",
    "X_test_vis_scaled = scaler_vis.transform(X_test_vis)\n",
    "\n",
    "# Train a KNN classifier on the two-feature dataset using the best parameters from grid search\n",
    "knn_vis = KNeighborsClassifier(n_neighbors=grid_search.best_params_['n_neighbors'],\n",
    "                               metric=grid_search.best_params_['metric'])\n",
    "knn_vis.fit(X_train_vis_scaled, y_train_vis)\n",
    "\n",
    "# Create a mesh grid for plotting decision boundaries\n",
    "x_min, x_max = X_train_vis_scaled[:, 0].min() - 1, X_train_vis_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_train_vis_scaled[:, 1].min() - 1, X_train_vis_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "# Predict class labels for each point in the mesh grid\n",
    "Z = knn_vis.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundaries along with the training points\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.coolwarm)\n",
    "scatter = plt.scatter(\n",
    "    X_train_vis_scaled[:, 0], X_train_vis_scaled[:, 1],\n",
    "    c=y_train_vis, s=20, edgecolor='k', cmap=plt.cm.coolwarm\n",
    ")\n",
    "plt.title('Decision Boundary with KNN')\n",
    "plt.xlabel(features[0])\n",
    "plt.ylabel(features[1])\n",
    "\n",
    "# Automatic legend based on unique classes in y_train_vis\n",
    "for i, class_name in zip(np.unique(y_train_vis), iris.target_names):\n",
    "    plt.scatter([], [], c=plt.cm.coolwarm(i / 2), label=class_name)\n",
    "plt.legend(title=\"Classes\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore test accuracy for different values of k using the two selected features\n",
    "k_values = range(1, 16)\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn_temp = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_temp.fit(X_train_vis_scaled, y_train_vis)\n",
    "    pred_temp = knn_temp.predict(X_test_vis_scaled)\n",
    "    accuracies.append(accuracy_score(y_test_vis, pred_temp))\n",
    "\n",
    "# Plot test accuracy vs. k\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(k_values, accuracies, marker='o')\n",
    "best_k = k_values[np.argmax(accuracies)]\n",
    "best_acc = max(accuracies)\n",
    "plt.annotate(f'Best k={best_k}\\nAcc={best_acc:.2f}', xy=(best_k, best_acc), \n",
    "             xytext=(best_k+1, best_acc-0.05),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "plt.title('K-value Selection: Test Accuracy vs. Number of Neighbors')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xticks(k_values)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "#### Baseline Comparison\n",
    "\n",
    "For reference, a simple baseline classifier that always predicts the majority class would achieve an accuracy of approximately 33% (since the Iris dataset is balanced). Our KNN model significantly outperforms this baseline, demonstrating its effectiveness.\n",
    "\n",
    "The KNN algorithm performed well on the Iris dataset, demonstrating clear decision boundaries in the low-dimensional visualization. Some key points to note:\n",
    "\n",
    "**Strengths:**\n",
    "- Simple to implement and interpret.\n",
    "- No assumptions about the underlying data distribution.\n",
    "\n",
    "**Weaknesses:**\n",
    "- Computationally expensive for very large datasets due to the distance computations.\n",
    "- Sensitive to the choice of k and feature scaling.\n",
    "\n",
    "While more sophisticated algorithms like Support Vector Machines or Decision Trees might capture complex patterns better, KNN remains useful for its simplicity and interpretability in many real-world tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this project, we showcased a complete machine learning workflow using the K-Nearest Neighbors (KNN) algorithm. We:\n",
    "\n",
    "- Loaded and explored the Iris dataset\n",
    "- Preprocessed the data (including scaling and train-test splitting)\n",
    "- Explained the mathematical foundation of KNN\n",
    "- Trained and tuned the model, evaluating it with several performance metrics\n",
    "- Analyzed model behavior using decision boundary and k-value performance plots\n",
    "\n",
    "The KNN algorithm, despite its simplicity, demonstrated effective performance on the dataset. Future work could involve applying dimensionality reduction, testing KNN on larger and more complex datasets, or comparing it with alternative classifiers.\n",
    "\n",
    "#### Next Steps\n",
    "\n",
    "- Apply dimensionality reduction (e.g., PCA) before KNN to visualize in lower dimensions.\n",
    "- Test KNN on larger or more complex datasets.\n",
    "- Compare KNNâ€™s performance with other classifiers such as Logistic Regression or Decision Trees.\n",
    "- Explore advanced hyperparameter tuning and cross-validation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. [Scikit-learn Documentation](https://scikit-learn.org/stable/)\n",
    "2. MÃ¼ller, A. C., & Guido, S. (2016). *Introduction to Machine Learning with Python*.\n",
    "3. [Iris Dataset - UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/iris)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
